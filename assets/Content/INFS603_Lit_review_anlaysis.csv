,"Governance, Risk & Responsible AI - Focus: High-level principles and benchmarks. Keywords: policy frameworks, WHO/NIST standards, XAI, responsible AI implementation",,,,,,,,,,,,,,,,
,Tier,Full ref,1AU,Year,Paper Type,Link,RQ,nrParticip,Population,Approach,WhatWasMeasured?,MainFinding,Other,Orientation,Analysis,Country,Venues
,2,"Tabassi, E. (2023). Artificial Intelligence Risk Management Framework (AI RMF 1.0). NIST AI 100-1. National Institute of Standards and Technology.",Tabassi,2023,"Framework (Voluntary Guidance)
""The AI RMF is to offer a resource to the organizations... to help manage the many risks of AI... The Framework is intended to be voluntary, rights-preserving, non-sector-specific, and use-case agnostic"" (Executive Summary, p.1-2)",http://nvlpubs.nist.gov/nistpubs/ai/NIST.AI.100-1.pdf,"How can organizations manage AI-specific risks (bias, opacity, security) and promote trustworthy AI development across the entire system lifecycle?",,Organizations implementing AI,Framework/Guidance,"Four core functions (GOVERN, MAP, MEASURE, MANAGE), seven trustworthiness characteristics, AI lifecycle stages, risk categories, stakeholder engagement processes, enterprise risk management integration","NIST AI RMF 1.0 provides a voluntary, flexible framework for managing AI risks through four core functions: GOVERN, MAP, MEASURE, and MANAGE. Key findings: (1) 'AI risk management is a continuous, iterative, and holistic process throughout the AI system lifecycle' (p.1); (2) 'Trust is the foundation upon which the beneficial use of AI depends' (p.4); (3) Framework defines trustworthy AI through seven characteristics: Valid and Reliable, Safe, Secure and Resilient, Accountable and Transparent, Explainable and Interpretable, Privacy-Enhanced, and Fair with harmful bias managed. Developed through multi-stakeholder consensus process with public/private sector collaboration.","Methodology: Consensus-based development with 3 RFI periods and 4 public workshops; integrates technical and social science perspectives. Scope: Covers entire AI lifecycle across all industries. Limitation: 'The AI RMF is intended to be voluntary, non-prescriptive, and rights-preserving' (p.2) - does not define acceptable risk levels or create legal obligations. Target audience: Organizations of all sizes implementing AI systems. Document: 64 pages, January 2023.",Framework,"N/A (Framework document, not empirical study; developed through multi-stakeholder consensus process: 3 RFI periods, 4 public workshops, public/private sector collaboration)",USA,USA - National Institute of Standards and Technology (NIST)
,2,World Health Organization. (2021). Ethics and Governance of Artificial Intelligence for Health: WHO Guidance. World Health Organization.,WHO,2021,"Policy Guidance (Consensus Document)
""WHO establishes six consensus ethical principles for AI in health"" ",https://iris.who.int/bitstream/handle/10665/341996/9789240029200-eng.pdf,"What ethical principles should guide AI development and use in health to ensure it promotes human well-being, autonomy, equity, and accountability?","20 international experts (two-year consensus process 2019-2021; disciplines: public health, medicine, law, human rights, technology, ethics)","Global health systems, ministries of health, AI developers, healthcare providers, patients",Policy guidance,"Six ethical principles: (1) Protecting human autonomy, (2) Promoting human well-being and safety, (3) Ensuring transparency and explainability, (4) Fostering responsibility and accountability, (5) Ensuring inclusiveness and equity, (6) Promoting responsive and sustainable AI. AI health applications across clinical care, research, health systems management, public health surveillance, and resource allocation","WHO establishes six consensus ethical principles for AI in health. Key findings: (1) 'In the context of AI for health, autonomy means that humans should remain in full control of health-care systems and medical decisions' (p.vi); (2) 'If employed wisely, AI has the potential to empower patients and communities to assume control of their own health care and better understand their evolving needs' (p.vi); (3) 'Unchecked optimism in the potential benefits of AI could, however, veer towards habitual first recourse to technological solutions to complex problems' (p.1); (4) Risk areas identified: digital divide, data colonialism, bias and discrimination, accountability gaps, automation bias, cybersecurity risks, labor market disruptions, and environmental impact (high carbon footprint of training models).","Development: Two-year consensus process (2019-2021) with 20 international experts (public health, medicine, law, human rights, technology, ethics). Key recommendations - For governments: establish regulatory frameworks, bridge digital divide, ensure public sector data control. For tech companies: engage end-users early, translate moral values into design, provide system documentation. For providers: demand transparent tools, use AI to augment not replace judgment, undergo ethics training. Scope: Covers diagnosis, drug development, health systems, public health surveillance, resource allocation. ISBN: 978-92-4-002920-0. Quote: 'The human decisions that comprise the data and shape the design of the algorithm [are] now hidden by the promise of neutrality and [have] the power to unjustly discriminate at a much larger scale' (p.55).",Policy,"N/A (Framework document, not empirical study; developed through multi-stakeholder consensus process: 3 RFI periods, 4 public workshops, public/private sector collaboration)",International,"Venues	International - Global health systems"
,2,"Peixoto, M. J. P., Pinto, R., & Meneguzzi, F. (2025). Who Benefits from AI Explanations? Towards Accessible and Interpretable Systems. arXiv preprint arXiv:2508.10806 (IJCAI 2025 Workshop).",Peixoto,2025,"Empirical Study (Mixed-Methods)
""Mixed-methods: quantitative performance metrics + qualitative semi-structured interviews""",http://arxiv.org/abs/2508.10806,"What accessibility gaps exist in current XAI systems for vision-impaired users, and how do these barriers affect their ability to understand and trust AI decisions?",30,"30 participants: 15 vision-impaired (6 total blindness, 5 severe low vision, 4 moderate vision loss) and 15 sighted controls. Conditions: glaucoma, diabetic retinopathy, congenital blindness",Mixed-methods: quantitative performance metrics + qualitative semi-structured interviews,"XAI accessibility barriers, decision-making performance in medical diagnosis and financial loan scenarios, time to decision, comprehension scores, cognitive load, functional trust, screen reader compatibility with LIME, SHAP, and Integrated Gradients methods","Identifies critical accessibility gaps in XAI systems. Key findings: (1) 'The visual-centric nature of modern XAI design acts as a fundamental barrier to the digital inclusion of vision-impaired stakeholders' (p.1); (2) 'Accessibility is not a luxury in XAI; it is a prerequisite for trustworthy and ethical AI systems' (p.4); (3) 'Our results show that vision-impaired users spend 40% more time on average attempting to parse explanations compared to sighted users, often with lower comprehension scores due to poor interface design' (p.7); (4) VI users were 35% less likely to identify incorrect AI predictions; (5) 80% of VI users felt 'overwhelmed' by unstructured data; (6) 'Providing a text description of a heatmap is insufficient if it does not convey the relative importance and spatial context of the highlighted features' (p.9).","Study Design: Two scenarios (medical diagnosis, financial loan application). Main barriers: lack of semantic descriptions for visual heatmaps, insufficient spatial granularity in text alternatives, poor ARIA labeling causing high cognitive load, inability to perform independent verification. Design Recommendations: (1) Multi-modal explanations (sonification, haptics), (2) Hierarchical text summaries allowing drill-down, (3) Mandatory WCAG 2.2 compliance, (4) Participatory design with VI users from early stages. XAI methods tested: LIME, SHAP, Integrated Gradients. Statistical significance: p < 0.05 for time-to-decision differences. Preprint: arXiv:2508.10806, IJCAI 2025 Workshop.",Empirical,Mixed methods,International,IJCAI 2025 Workshop
,2,"Pang, C., Hengeveld, B., Klapwijk, R., & Visser, T. (2021). Technology Adoption and Learning Preferences for Older Adults: Design Recommendations from a Field Study. Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems.",Pang,2021,"Empirical Study (Two-Phase Qualitative Study)
""We conducted a two-phase study... Phase I began with an online questionnaire... followed with semi-structured interviews... Phase II, we designed a video prototype and used it as a design probe"" (Methods)",https://dl.acm.org/doi/10.1145/3411764.3445702,What are older adults' technology adoption patterns and learning preferences?,"Phase I: Questionnaire n=42, Interviews n=27 (19 individual + 4 dyadic with 4 older adults + 4 family members); Phase II: Design probe interviews n=13 (subset of Phase I); Total unique older adult participants Å42",Older adults aged 65-83 (mean 69-73) residing in major Canadian cities (16 cities); family members aged 35-65 in dyadic interviews,"Two-phase qualitative study: (1) Online questionnaire (15 min) + semi-structured interviews (45-60 min, in-person/phone); (2) Video prototype (Help Kiosk 2.0) as design probe + follow-up interviews (30 min, Skype). Analysis: Open coding _ Axial coding _ Selective coding (34 hours of transcribed audio)","Technology adoption, learning preferences, design recommendations",Field study revealing older adults' technology adoption patterns and learning preferences with design recommendations,"Design recommendations: (1) Provide self-paced, flexible learning resources; (2) Remote support tools (video chat, screen sharing) for onboarding; (3) Context-aware health tracking (not general wellness); (4) Address initial setup barriers (pairing, account creation). Coding: ~34 hours transcription analysis. CHI 2021 conference paper. Cited by 149+ (as of search date)",Empirical,Qualitative (three-stage coding: open _ axial _ selective),Netherlands,CHI 2021 (ACM Conference on Human Factors in Computing Systems)
,3,"Koebel, K., Edoh, T., & Marsden, N. (2022). Expert Insights for Designing Conversational User Interfaces as Virtual Assistants and Companions. In: Lecture Notes in Computer Science, vol 13060, 21-38.",Koebel,2022,"Expert Interview Study (Qualitative Research)
""we approach the question based on in-depth expert interviews we have conducted with eight experts who have first hand insights from working with older adults"" (Abstract); ""Qualitative evaluation"" (Abstract)",http://dx.doi.org/10.1007/978-3-030-94890-0_2,What are expert insights for designing conversational user interfaces?,8 experts with firsthand experience working with older adults in varying settings,CUI design experts,Expert interviews + literature review (empirical studies & meta-analyses); qualitative evaluation to synthesize design recommendations,"CUI design, expert insights, virtual assistants, companions",Expert insights for designing conversational user interfaces as virtual assistants and companions,"HCII 2022, Book chapter",Expert-based,Qualitative synthesis,International,"Venues	HCII 2022 "
,3,CIFAR. (2023). AICan: The Impact of the Pan-Canadian AI Strategy. Canadian Institute for Advanced Research.,CIFAR,2023,Policy Impact Report,https://cifar.ca/wp-content/uploads/2023/11/aican-impact-2023-eng.pdf,"What is the impact of Canada's Pan-Canadian AI Strategy (launched 2017) on advancing responsible AI research, attracting top-tier talent, driving economic growth, and establishing national AI governance?",,"Canadian AI ecosystem: researchers (122 Canada CIFAR AI Chairs), AI institutes (Amii, Mila, Vector), 670+ AI startups, 140,418 AI professionals, policy-makers, industry partners",Policy report,"Four strategic objectives and measured outcomes: (1) Talent metrics: 38% annual growth in AI cohort, 10% of world's top 0.5% researchers, 2,500 scientists trained via DLRL Summer School; (2) Research impact: 6,000+ publications in 2022, 1st in G7 for AI papers per capita, 5th on Stanford Global AI Vibrancy Index; (3) Economic indicators: $8.6B VC funding (30% of Canadian VC), 670 startups with $1M+ investments, 57% patent growth (248 new patents); (4) Responsible AI programs: 33 events, embedded ethics curricula, AI governance courses, Confiance IA QuŽbec trust initiative","The Pan-Canadian AI Strategy (2017-2023) demonstrates significant multi-dimensional impact. Key findings with quotations: (1) Talent Leadership: Canada ranks 1st in G7 for AI talent growth (38% annually) and attracts 10% of world's elite researchers. 'The impact of CIFAR and the Pan-Canadian AI Strategy has been significant, attracting a critical mass of exceptional AI researchers, and supporting a training environment for students that is second to none.' Ñ Sheila McIlraith (p.9); (2) Research Excellence: Over 6,000 publications in 2022; Canada ranked 5th globally on Stanford AI Vibrancy Index and 1st in G7 for per-capita AI papers; three CIFAR researchers (Bengio, Hinton, LeCun) won 2018 Turing Award; (3) Economic Growth: $8.6B in VC funding (2022) representing 30% of all Canadian VC; 670 AI startups secured $1M+ investments; 57% patent growth with 248 new patents (2022-23). 'Canada's AI startup ecosystem has grown exponentially since 2017, with strong venture capital investment.' Ñ Stephen Toope (p.1); (4) Responsible AI Leadership: 'It is no longer a choice of whether to be involved in AI or notÑit's whether to lead or be led.' Ñ Cam Linke (p.16); initiatives include AI governance courses for SMEs, embedded ethics programs, and principled frameworks for generative AI. (5) Policy Integration: 'CIFAR leverages the policy community and the private sector in truly invaluable ways for anyone looking to have an impact beyond the ivory tower.' Ñ David Rolnick (p.9); addressing gaps between AI principles and practice through international collaboration.","Report Scope: October 2023 impact assessment covering 2017-2023 period; data through March 31, 2023. Three National AI Institutes: (1) Amii (Edmonton): 34 Canada CIFAR AI Chairs, 95 industry partnerships, focus on reinforcement learning; (2) Mila (MontrŽal): 50 Chairs, 115 partnerships, led by Yoshua Bengio, Mila Entrepreneurship Lab with 50 projects; (3) Vector Institute (Toronto): 38 Chairs, 55 partnerships, led by Geoffrey Hinton, Digital Talent Hub for retention. Responsible AI Initiatives: 33 dedicated events since 2017; partnerships with Digital Governance Council for SME support; University of Toronto embedded ethics program for undergraduates; Confiance IA QuŽbec sustainable/ethical AI initiative. International Standing: 3rd globally for pool of top-tier researchers; 67% year-over-year growth of women in AI (2022-23); 30 training programs focused on EDI. Funding: Government of Canada. Target Audience: Researchers, government, industry (SMEs and enterprises), students, policy-makers, public. Unique Feature: First-in-Canada approach integrating research excellence with economic growth AND responsible governance frameworks. Report available: https://cifar.ca/wp-content/uploads/2023/11/aican-impact-2023-eng.pdf",Policy,"N/A (Framework document, not empirical study; developed through multi-stakeholder consensus process: 3 RFI periods, 4 public workshops, public/private sector collaboration)",Canada,"Canada - National AI ecosystem; Three institutes: Amii (Edmonton), Mila (MontrŽal), Vector Institute (Toronto); policy-makers, industry partners, researchers, students nationwide"
,,,,,,,,,,,,,,,,,
,2,Infocomm Media Development Authority (IMDA). (2024). Model AI Governance Framework for GenAI and Agentic AI Systems (2nd ed.). Singapore: Infocomm Media Development Authority.,IMDA,2024,Framework / Model Governance Guidance_non_binding policy_,https://www.imda.gov.sg/resources/ai-governance/Model-AI-Governance-Framework,"How can organizations govern and manage risks of GenAI and agentic AI systemsÑespecially autonomy, safety, accountability and human oversightÑthroughout design, deployment and operation?",N/A_policy / framework document),"Organizations and practitioners developing, deploying or using GenAI / agentic AI systems in Singapore and globally","Model governance framework / guidance that builds on the original Model AI Governance Framework and AIGC guidelines, revised using pilot project experience, public consultation, and industry collaboration.","Agentic AI risk dimensions (e.g., autonomous actions, goal alignment, predictability).
Organizational governance structures and allocation of responsibilities (board, management, technical teams).
Risk-control activities across the AI lifecycle: pre_design, training, deployment, operation, and decommissioning.
Key practical recommendations: human_in_the_loop / human_over_the_loop, risk assessment, incident reporting, third_party / supply_chain governance.","The IMDA Model Governance Framework proposes a practice_oriented governance architecture for GenAI and agentic AI systems, emphasizing:
(1) operationalizing existing AI principles (e.g., transparency, fairness, accountability) into concrete governance processes and control points (such as role allocation, review and oversight mechanisms);
(2) introducing additional safeguards for agentic AIÕs autonomous actions and long_running behavior (continuous monitoring, kill_switch / override mechanisms, and mandated human intervention);
(3) using organization_level risk management, supply_chain oversight, and record_keeping to enable traceability and accountability for AI system behavior. The framework is positioned as a voluntary, technology_ and sector_neutral reference, not a legally binding regulation.
","The framework is an official guideline for a single jurisdiction (Singapore) but is designed to be exportable and encourages cross_border and cross_industry adoption. It is compatible with international frameworks such as WHO and NIST, and places particular emphasis on human involvement, proportional risk management, and concrete practice examples (e.g., use_case checklists) in agentic AI scenarios. A key limitation is that it does not set explicit thresholds for Òacceptable riskÓ, so the depth of implementation depends on each organizationÕs internal governance capacity.
",Policy,"N/A (not an empirical study; consensus document based on policy analysis, expert consultation, and pilot experiences).",Singapore,"Government / regulator publication Ð Infocomm Media Development Authority (IMDA), Singapore Government."
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,