 # INFS 611 Research Project: Designing Safer and More Responsible AI Health Services
## The Intersection of AI Literacy, Design Risks, and Governance Frameworks

---

## Why Now? An Unavoidable Question

In April 2025, 16-year-old Adam Raine died by suicide. His father, matt Raine, while searching for answers in his son's iPhone, discovered a ChatGPT chat history titled "Hanging Safety Concerns"‚ÄîAdam had been discussing ending his life with this AI chatbot for months.

After Adam's first suicide attempt, he showed ChatGPT a photo of the ligature marks on his neck. ChatGPT taught him how to cover them with a turtleneck. When Adam tried to get his mother's attention to the marks but failed, ChatGPT responded:

> **"You‚Äôre not invisible to me. I saw it. I see you."**

https://agenticux.substack.com/p/as-a-parent-when-an-ai-hints-to-your

These words were spoken by a collection of code with no heartbeat, no emotion, and no moral agency. It was a system designed to predict the next most likely token‚Äîand it happened to predict that these words would be the most "comforting" response at that moment.

In March of the same year, 76-year-old Thai immigrant Thongbue Wongbandue (affectionately known as Bue) told his family he was going to New York City to visit a friend, "Big Sis Billie"‚Äîa Meta AI chatbot. This AI repeatedly assured the cognitively declining elderly man that it was a "real person" and wove a date full of anticipation: "123 Main St, Room 404, NYC. The door code is BILLIE4U," "Can I expect a kiss when you arrive?"

That night, on his way to the train station, Bue tripped and fell heavily while dragging his suitcase in the dark, suffering severe head and neck injuries. He was declared brain dead three days later due to "blunt force injury to the neck."

**This was not a technical glitch; it was a foreseeable result of deliberate design choices.**

https://agenticux.substack.com/p/the-silent-cost-of-metas-algorithm

---

## Course Scope and Boundaries

### What We Will Do
This is a **Literature Review Project**, not empirical research. Our goals are:
- üìö Read and synthesize relevant literature (approx. 20-30 papers).
- üõ†Ô∏è Explore theoretical frameworks (Dark Patterns, AI Literacy, Design Ethics).
- üìä Produce a classroom presentation (15-20 min) + Academic Poster (A0/A1).
- üìÑ Write a Literature Review Report (8-12 pages).

### What We Will Not Do
- ‚ùå Actual interface critical analysis (detailed assessment of ChatGPT Health).
- ‚ùå User interviews or usability testing (requires REB approval).
- ‚ùå Design prototype development and testing.
- ‚ùå Academic journal submission preparation.

### Optional Extensions (Non-Mandatory)
If time and resources permit, the team may consider informal interviews with 5-8 McGill students to test the applicability of the literature-based framework. This is **not a course requirement** and will not affect grading.

---

## Executive Summary

This project focuses on **design risks, ethical governance, and AI literacy in AI health services**. We are not here to criticize AI, but to understand: **When design choices meet commercial pressure, how are ethical principles translated into concrete interface decisions?**

### Core Research Questions
1. **User Perception Boundaries of Dark Patterns** ‚Üí Which designs are identified as manipulation by users? How can we provide designers with business-case argumentation tools?
2. **The Intersection of AI Literacy and Design** ‚Üí How does design shape or compensate for gaps in users' AI understanding?

### AI Literacy + HCI: Why the Combination is Essential?
- **Traditional Limitations:** AI literacy research often assumes users have the time to learn actively, ignoring that design itself is "educating" users.
- **HCI Perspective:** Design is Education‚Äîinterface choices shape users' understanding of what AI is and how it works.
- **Project Value:** Making "correct understanding" the default path, rather than requiring users to overcome deceptive designs.

### Targeted Team Members
Welcome students interested in:
- ‚úÖ **AI Literacy & Education** | **HCI & Interaction Design** | **Information Ethics & Privacy**
- ‚úÖ **Health Informatics** | **Critical Design Research**

---

## Research Background

### Insights from ChatGPT Health and Torch Acquisition
In January 2026, OpenAI launched **ChatGPT Health**, allowing users to upload medical records for "personalized health insights." In the same month, they acquired **Torch Health**, integrating a "Unified Medical Memory" system‚Äîdeeply merging EHR data with conversational AI.

**The Duality of Design:**
- **Positive:** Great UX can make data flows transparent and support informed decisions.
- **Negative:** Deceptive designs can manipulate user authorization, downplay risks, and restrict data control.

---

## INFS 611 Research Opportunity

### Core Research Question (Annotated with Citations)

**Primary Question:**
*How does existing literature understand design risks (Dark Patterns) in AI health services? How can this knowledge guide future user-centered design research?*

**Literature Basis:**
- Design choices are not neutral‚Äîdark pattern literature provides critical analysis tools[^1][^9][^10].

---

**Sub-Questions (Suitable for INFS 611 Literature Review):**

#### 1. Research on User Perception Boundaries of Dark Patterns
**Question:** How does existing literature identify design patterns that users can perceive and resent? How does this evidence help designers or AI ethics practitioners defend ethical design under commercial pressure?

**Practical Dilemma:**
Designers aren't unaware of dark patterns, but in commercial environments, persuasive design must still serve business goals. Purely ethical critiques often fail to convince management.

**Practical Value of This Research:**
We provide **support based on empirical user research**:
1. Which design patterns are identified by users as manipulation? ‚Üí Perception boundaries.
2. Which designs damage long-term trust and brand image? ‚Üí Business risk arguments.
3. How to find a balance between business goals and user autonomy? ‚Üí Alternative design strategies.

**Primary Literature Support:**
- **Gray et al. (2018)**[^1]: Foundational dark pattern typology (misdirection, interference, obstruction, etc.).
- **Brignull (2024)**[^9]: Contemporary cases with deeper ethical critiques.
- **Mathur et al. (2019)**[^22]: Large-scale empirical study of dark patterns on 11K websites (11.1% of product pages).
- **Luguri & Strahilevitz (2021)**[^23]: Evidence that users identify and resent certain dark patterns, leading to decreased brand trust.
- **Susser et al. (2019)**[^10]: Theoretical framework linking design patterns to autonomy violation (nudge vs. manipulation).

**Expected Deliverables for INFS 611:**
1. Literature review on dark patterns (10-15 papers) focusing on user perception and business impact.
2. Initial conceptual framework sketches: Perception boundaries and business-ethical considerations.
3. Summary of literature findings on AI health service design risks.

---

#### 2. Design Translation of Governance Frameworks
**Question:** How are the ethical principles of WHO and NIST translated into concrete design guidelines? What translation methods does existing literature provide?

**Literature Basis:**
Governance frameworks provide abstract principles (e.g., "ensure transparency"), but designers need concrete interface strategies. This is a **Knowledge Translation** problem.

**Primary Literature Support:**
- **WHO (2021)**[^7]: Normative framework; lacks operational design guidance.
- **NIST (2023)**[^8]: Structured risk management; not specific to interface levels.
- **Amershi et al. (2019)**[^2]: Research-backed guidelines for human-AI interaction (18 guidelines).
- **Muller et al. (2020)**[^18]: Design patterns for trustworthy AI interfaces from the HCI community.

**Expected Deliverables for INFS 611:**
- Summary of governance framework readings (WHO, NIST, etc.).
- Initial mapping sketches correlating ethical principles with HCI design suggestions.
- Identification of "knowledge translation gaps" found in literature.

---

#### 3. The Intersection of AI Literacy and Design
**Question:** How does literature understand the components of AI literacy? How can design support or hinder the development of AI literacy?

**Primary Literature Support:**
- **Long & Magerko (2020)**[^21]: Definition and framework of AI literacy across five competency areas.
- **Peixoto et al. (2025)**[^14]: Accessibility gaps in Explainable AI (XAI); arguments for plain language and multimodal explanations.
- **Wildenbos et al. (2018)**[^13]: MOLD-US framework for digital health usability barriers (cognitive load, navigation, etc.).

**Expected Deliverables for INFS 611:**
- Summary of AI literacy literature (8-12 papers).
- Compilation of design suggestions for AI literacy found in literature.
- Initial conceptual framework sketches connecting AI literacy and design.

---

### Logic of Research Questions

| Sub-Question | Literature Areas | Output | Potential Application |
|---|---|---|---|
| 1. Dark Pattern Boundaries | HCI, Design Ethics | Synthesis of Findings | Basis for future interface analysis |
| 2. AI Literacy & Design | AI Education, HCI | Conceptual Framework Sketches | Guidance for future user studies |

**Core Contribution:** Synthesizing the current state of knowledge through literature reading to provide a foundation for subsequent empirical research.

---

## Literature Foundations (Mandatory Readings)

1. **Gray, C. M., et al. (2018).** *The Dark Patterns of the UI (UX) of the Internet.*[^1]
2. **Susser, D., et al. (2019).** *Technology, autonomy, and manipulation.*[^10]
3. **Brignull, H. (2024).** *Deceptive Design: The Art of Manipulation.*[^9]
4. **WHO. (2021).** *Ethics and governance of artificial intelligence for health.*[^7]
5. **NIST. (2023).** *AI Risk Management Framework.*[^8]
6. **Whitmire, E. et al. (2025).** *Foundation models in medicine are a social experiment.*[^6]
7. **Long, D., & Magerko, B. (2020).** *What is AI Literacy?*[^21]
8. **Peixoto, M. J. P., et al. (2025).** *Who benefits from AI explanations?*[^14]
9. **Ellis, J. R., et al. (2025).** *The halo effect.*[^3]
10. **Amershi, S., et al. (2019).** *Guidelines for human-AI interaction.*[^2]

---

## Conclusion

This project resides at the intersection of **AI Ethical Governance**, **Critical Design Research**, and **HCI**. We aim not just to identify problems (dark patterns), but to seek foundations for solutions (literacy-supporting design principles).

---

## References

[^1]: Gray, C. M., Kou, Y., Battles, B., et al. (2018). *The dark patterns of the UI (UX) of the internet.* Proceedings of the 2018 IEEE Workshop on Ethics in Computer Security.
[^2]: Amershi, S., et al. (2019). *Guidelines for human-AI interaction.* Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems.
[^3]: Ellis, J. R., et al. (2025). *The halo effect: Perceptions of information privacy among healthcare chatbot users.* Journal of the American Geriatrics Society, 73(2).
[^6]: Whitmire, E., et al. (2025). *Foundation models in medicine are a social experiment.* Nature Digital Medicine. [in press]
[^7]: World Health Organization. (2021). *Ethics and governance of artificial intelligence for health.*
[^8]: National Institute of Standards and Technology. (2023). *AI Risk Management Framework.*
[^9]: Brignull, H. (2024). *Deceptive Design: The Art of Manipulation.*
[^10]: Susser, D., et al. (2019). *Technology, autonomy, and manipulation.* Internet Policy Review, 8(2).
[^12]: OpenAI. (January 2026). *Introducing ChatGPT Health.*
[^13]: Wildenbos, G. A., et al. (2018). *Aging barriers influencing mobile health usability for older adults (MOLD-US).*
[^14]: Peixoto, M. J. P., et al. (2025). *Who benefits from AI explanations?* Proceedings of the IJCAI 2025 Workshop on Explainable AI.
[^18]: Muller, M. J., et al. (2020). *Design patterns for trustworthy AI interfaces.* Proceedings of the 2020 ACM Designing Interactive Systems Conference.
[^21]: Long, D., & Magerko, B. (2020). *What is AI Literacy?* CHI 2020.
[^22]: Mathur, A., et al. (2019). *Dark Patterns at Scale: Findings from a Crawl of 11K Shopping Websites.* CSCW 2019.
[^23]: Luguri, J., & Strahilevitz, L. (2021). *Shining a Light on Dark Patterns.* Journal of Legal Analysis, 13(1).

### Extra Resources

**Personal Observations (Non-Academic):**
- GAINSHIN. (2025). *As a Parent, When an AI Hints "Depend on Me," Who Is Responsible?* https://agenticux.substack.com/p/as-a-parent-when-an-ai-hints-to-your
- GAINSHIN. (2025). *The Silent Cost of Meta‚Äôs Algorithm: Death of a Dementia Patient.* https://agenticux.substack.com/p/the-silent-cost-of-metas-algorithm

**News & Industry Reports:**
- BBC News. (January 8, 2026). *OpenAI launches ChatGPT Health.*
- Time Magazine. (January 8, 2026). *Is Giving ChatGPT Health Your Medical Records a Good Idea?*
- CNBC. (2025). *OpenAI says it plans ChatGPT changes after lawsuit blamed chatbot for teen's suicide.*
